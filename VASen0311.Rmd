---
title: "#VASEN Analysis"
author: "Georges Colbert"
date: "7/16/2018"
output:
  word_document: default
  html_document: default
---

---
output:
  word_document: default
  html_document: default
---





```{r echo=FALSE}
library(twitteR)
library(wordcloud)
library(tm)
library(SnowballC)
library(readr)
 library(dplyr)
library(ggplot2) 

 #VA.tweetdf <- read_csv("/Users/georgesericcolbert/Stew.tweetdf.28.03.csv")
 #VA.tweetdf <- read_csv("/Users/georgesericcolbert/VA.tweetdf.27.2.csv")
 VA.tweetdf <- read_csv("/Users/georgesericcolbert/VA.tweetdf.August.03.13.csv")


```

#Total Number of tweets
```{r }
 
 
 
 
 nrow(VA.tweetdf)

```


# Number of Tweets per Day

```{r,echo=FALSE}

 library(ggplot2)
  library(scales)
  
  
  #Plotting how often coreystewartva is tweeted per day
   VA.tweetdf$Date <- as.Date(VA.tweetdf$created)
  time.freq <- as.data.frame(table(VA.tweetdf$Date))
  
  
  colnames(time.freq) <- c("Date","TweetsperDay")
  time.freq$Date <- as.Date(time.freq$Date)
  
  ggplot(data = time.freq,aes(Date,TweetsperDay))+geom_bar(stat="identity", fill="red")+scale_x_date(labels = date_format("%Y-%m-%d"))
  
  
  
  


```



# How many are retweets?
```{r }
 
 
 ##### How many are retweets?
 
 table(VA.tweetdf$isRetweet)

```

```{r}

 q <- grepl("CoreyStewartVA|Corey|corey|Stewart|stewart", VA.tweetdf$text)

 q <- table(q)
q <- as.data.frame(q)

colnames(q) <- c("Mentions Corey Stewart", "Count")

q

ggplot(q, aes(`Mentions Corey Stewart`, Count))+geom_bar(stat="identity", fill="red")+ ggtitle("Mentions Corey Stewart?")


```







```{r}

p <- grepl("Tim|tim|kaine|Kaine|timkaine", VA.tweetdf$text)

 p <- table(p)
 
 p <- as.data.frame(p)
 
colnames(p) <- c("Mentions Tim Kaine", "Count")

ggplot(p, aes(`Mentions Tim Kaine`, Count))+geom_bar(stat="identity", fill="blue")+ ggtitle("Mentions Tim Kaine?")

```




### list of top 6 favorite tweets of the period

```{r echo=FALSE}

### lis of top 6 favorite tweets 


  temp <- VA.tweetdf[order(VA.tweetdf$favoriteCount, decreasing=TRUE),]

   

 I <-  head(select(temp,text,favoriteCount,created,screenName))
    


as.data.frame(I)

 
```



# top 6 retweeted tweets of the period
```{r echo=FALSE}


# top 6 retweeted tweet 

 


   temp <- filter(temp, isRetweet == 'FALSE')
  temp <- temp[order(temp$retweetCount, decreasing=TRUE),]
  
  I <- head(select(temp,text,retweetCount,created,screenName))
  
  as.data.frame(I)
  
  
  
```


### Top ten retweets overall

In this case, there were tweets that were creeated before the week that were still being shared on Twitter and accumulating retweets.

```{r }
 I <- VA.tweetdf[order(VA.tweetdf$retweetCount, decreasing=TRUE),]


 I <- I[!duplicated(I$text),]


I <- head(select(I,text,retweetCount,created,screenName,isRetweet),10)
 
  as.data.frame(I)
```


## accounts that have the most total Retweets

```{r echo=FALSE}


  ### accounts that have the most total Retweets
  temp <- filter(VA.tweetdf, isRetweet == 'FALSE')
   x <- aggregate(temp$retweetCount, by=list(screenName=temp$screenName), FUN=sum)
  x <- x[order(x$x, decreasing=TRUE),]
  head(x)





```

## Most Active Accounts (including Retweets)


```{r echo=FALSE}
 ### Most Active Accounts (including Retweets)
I <-  as.data.frame(table(VA.tweetdf$screenName))

I <- I[order(I$Freq, decreasing=TRUE),]


head(I)



```




## Most Active Accounts 

```{r echo=FALSE}
 ### Most Active Accounts 
I <- as.data.frame(table(temp$screenName))

I <- I[order(I$Freq, decreasing=TRUE),]

head(I)




```


#WORDCLOUD of most common used words 


```{r echo=FALSE}
#######
 tweet.Corpus <- Corpus(VectorSource(VA.tweetdf$text))
 tweet.Corpus <- tm_map(tweet.Corpus, removeWords, stopwords())
 
 remove_url <- function(x) gsub("http[^[:space:]]*","",x)
 tweet.Corpus <- tm_map(tweet.Corpus, content_transformer(remove_url))
 
 
 ##### remove anything other than english letters and space
 
 removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*","",x)
 tweet.Corpus <- tm_map(tweet.Corpus, content_transformer(removeNumPunct))
 tweet.Corpus <- tm_map(tweet.Corpus, removePunctuation)
 tweet.Corpus<- tm_map(tweet.Corpus, content_transformer(tolower))
 tweet.Corpus <- tm_map(tweet.Corpus, stripWhitespace)
 
 myStopWords <- c("virginia", "2018","senate", "vasen")
 
 tweet.Corpus <-tm_map(tweet.Corpus, removeWords,myStopWords)
 
 
 Corpus.copy <- tweet.Corpus
 
 
 
 tweet.Corpus <- tm_map(tweet.Corpus, stemDocument)   ### if stemDocument doesn't run, install the package 'SnowballC'
 #tweet.Corpus <- tm_map(tweet.Corpus, content_transformer(stemCompletion), dictionary = Corpus.copy)
 
 

 
 
 

 
 #### Specific words you may want to remove #Virginia, Republican, GOP, Democrat, 2018, Primary
 #myStopWords <- "Corey Stewart"
 #myStopWords <- c("Corey Stewart",  "Virginia", "Republican", "GOP", "Democrat", 2018, "Primary", " Corey", "Stewart", "coreystewartva","stewart","corey") 
 

 #tweet.Corpus <- tm_map(tweet.Corpus, PlainTextDocument)

 
 
 tdm <-  TermDocumentMatrix(tweet.Corpus)
 dtm <- DocumentTermMatrix(tweet.Corpus)
 
 
 #library(wordcloud)
 #wordcloud(tweet.Corpus, min.freq = 10, random.order = F)
 
 
 
 freq <- colSums(as.matrix(dtm)) 

 
 
 

 
 
 
 

 
 




```


#WORDCLOUD of most common used words (Top 40)

```{r}

 library(wordcloud)
 dark2 <- brewer.pal(6, "Dark2")
 wordcloud(names(freq), freq, min.freq = 50, rot.per=0.2, colors=dark2,random.order = F) 
 
 



```

#List of The Top 15 Frequently used Words


```{r echo=FALSE}

#List of The Top Ten Frequently used Words



wf <- data.frame(word=names(freq), freq=freq) 
  wf <- wf[order(wf$freq, decreasing=TRUE),]
  head(wf, n = 15)  
  


```


#Plot of Words that Appear at least 75 times

```{r echo=FALSE}
 library(ggplot2)   
  
  plot.tweet <- ggplot(subset(wf, freq>75), aes(x = reorder(word,-freq), y = freq)) +
    geom_bar(stat = "identity", fill="red") + 
    theme(axis.text.x=element_text(angle=45, hjust=1))
  plot.tweet   
  


```

## Words Associated with 

 To find out what words are associated with the candidates, I use a function called FindAssocs() on the dataset. For any given word, findAssocs() calculates its correlation with every other word in the dataset. Scores range from 0 to 1. A score of 1 means that two words always appear together, while a score of 0 means that they never appear together.

```{r}


# which words are associated with "Corey Stewart"
  
  findAssocs(tdm,"coreystewartva", 0.1)
  
  
  findAssocs(tdm,"timkain", 0.1)
  
  findAssocs(tdm,"trump",0.1 )

```





```{r echo=FALSE}

### import package "sentimentr"
library(sentimentr)


VA.tweetdf <- read_csv("/Users/georgesericcolbert/VA.tweetdf.August.08.14.csv")


i.VA.tweetdf <- VA.tweetdf
  
  
i.VA.tweetdf$text <- gsub("[^0-9A-Za-z///' ]", "",i.VA.tweetdf$text)
#### remove http link
i.VA.tweetdf$text <- gsub("http\\w+", "",i.VA.tweetdf$text)
#### remove rt
i.VA.tweetdf$text <- gsub("rt", "",i.VA.tweetdf$text)
### remove at
i.VA.tweetdf$text <- gsub("@\\w+", "",i.VA.tweetdf$text)

i.VA.tweetdf$text <- tolower(i.VA.tweetdf$text)


emo_R_tweets <- sentiment(i.VA.tweetdf$text)
#View(emo_R_tweets)

VA.tweetdf$sentiment <- emo_R_tweets$sentiment  ## attaching the sentiment measure to every tweet
#View(R.tweetdf)


```



#Top 6 Negative Tweets

```{r echo=FALSE}

### grabbing positive tweets, sample of 



#positive_tweets <- head(unique(VA.tweetdf[order(VA.tweetdf$sentiment, decreasing = T),c(2,18)]),10)
#positive_tweets

#grabbing negative tweets
negative_tweets <- head(unique(VA.tweetdf[order(emo_R_tweets$sentiment),c(2,18)]),10)
#negative_tweets

 I<- VA.tweetdf[order(VA.tweetdf$sentiment, decreasing = FALSE),]
 I<-   filter(I, isRetweet == 'FALSE')
 I <-  head(select(I,text,retweetCount,created,screenName))
 
as.data.frame(I)

```


#Top 6 Positive Tweets


```{r echo=FALSE}

### grabbing positive tweets, sample of 



positive_tweets <- head(unique(VA.tweetdf[order(VA.tweetdf$sentiment, decreasing = T),c(2,18)]),10)
#positive_tweets

#grabbing negative tweets
negative_tweets <- head(unique(VA.tweetdf[order(emo_R_tweets$sentiment),c(2,18)]),10)
#negative_tweets

 I<- VA.tweetdf[order(VA.tweetdf$sentiment, decreasing = TRUE),]
 I<-   filter(I, isRetweet == 'FALSE')
 I <-  head(select(I,text,retweetCount,created,screenName))
 
as.data.frame(I)

```

```{r echo=FALSE}
B <- VA.tweetdf
B$Date <- as.Date(VA.tweetdf$created)
  
B$polarity <- ifelse(B$sentiment > 0.05,"positive",ifelse(B$sentiment < -0.05,"negative","neutral"))

qplot(factor(polarity), data=B, geom="bar", fill=factor(polarity))+xlab("Polarity Categories") + ylab("Frequency") + ggtitle("Mentions Sentiments Overall ")


```
# WorldCloud of words used in top 50 most negative and top 50 most positive tweets

```{r echo=FALSE}

VA.tweetdf <- i.VA.tweetdf
VA.tweetdf$sentiment <- emo_R_tweets$sentiment
positive_tweets <- head(unique(VA.tweetdf[order(VA.tweetdf$sentiment, decreasing = T),c(2,18)]),50)
#positive_tweets

#grabbing negative tweets
negative_tweets <- head(unique(VA.tweetdf[order(emo_R_tweets$sentiment),c(2,18)]),50)

#negative_tweets


write.table(positive_tweets$text, file = "/Users/georgesericcolbert/Desktop/Projects/Political Tweets/WordCloud/positive.txt",sep = "")
write.table(negative_tweets$text, file = "/Users/georgesericcolbert/Desktop/Projects/Political Tweets/WordCloud/negative.txt",sep = "")

#### 
library(tm)
tweet.Corpus.2 <- Corpus(DirSource(directory = "/Users/georgesericcolbert/Desktop/Projects/Political Tweets/WordCloud"))
summary(tweet.Corpus.2)

library(tm)

clean.tweet.Corpus.2 <- tm_map(tweet.Corpus.2, tolower)
clean.tweet.Corpus.2 <- tm_map(clean.tweet.Corpus.2, removeWords, stopwords())
clean.tweet.Corpus.2 <- tm_map(clean.tweet.Corpus.2, content_transformer(removeNumPunct))
clean.tweet.Corpus.2 <- tm_map(clean.tweet.Corpus.2, removePunctuation)
clean.tweet.Corpus.2 <- tm_map(clean.tweet.Corpus.2, stripWhitespace)
clean.tweet.Corpus.2 <- tm_map(clean.tweet.Corpus.2, stemDocument)
 #myStopWords <- c( "2018","the","dont", "corey","stewart","coreystewava","stewa","scotttaylorva", "jackposobiec")
#myStopWords <- c( "2018","the","dont", "tim","kain","timkain")
 myStopWords <- c("virginia", "2018","senate", "vasen")

 clean.tweet.Corpus.2 <-tm_map(clean.tweet.Corpus.2, removeWords,myStopWords)

###### TermDocumentMatrix and DocumentTermMatrix do the same thing
tc2_tdm <- TermDocumentMatrix(clean.tweet.Corpus.2)

tc2_matrix <- as.matrix(tc2_tdm)


```

```{r}
colnames(tc2_matrix) <- c("negative Tweets", "Positive Tweets")
comparison.cloud(tc2_matrix, max.words = 24,colors=brewer.pal(ncol(tc2_matrix),"Dark2"), random.order = FALSE)



```

```{r echo=FALSE}

 b <-  B %>% group_by(Date) %>% count(polarity)
  
#df_melt = melt(b, id.vars = 'Date')

ggplot(b, aes(fill=polarity, y=n, x=Date)) + 
   geom_bar( stat="identity")+ ggtitle("Mentions Sentiment by Date ")+ylab("Count")
#ggplot(B,aes(x=Date, y=sentiment, fill=polarity)) + geom_bar(stat="identity",position = "stack")


ggplot(b, aes(fill=polarity, y=n, x=Date)) + 
   geom_bar( stat="identity",position = "fill")+ ggtitle("Mentions Sentiment by Date (%)")+ylab("Percentage")


```





```{r echo=FALSE}

hist(B$sentiment, breaks=20, col="red", main = "Distribution of Sentiment",xlab= "Sentiment Score")


```


